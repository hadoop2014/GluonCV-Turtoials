{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Train Mask RCNN end-to-end on MS COCO\n===========================================\n\nThis tutorial goes through the steps for training a Mask R-CNN [He17]_ instance segmentation model\nprovided by GluonCV.\n\nMask R-CNN is an extension to the Faster R-CNN [Ren15]_ object detection model.\nAs such, this tutorial is also an extension to :doc:`../examples_detection/train_faster_rcnn_voc`.\nWe will focus on the extra work on top of Faster R-CNN to show how to use GluonCV components\nto construct a Mask R-CNN model.\n\nIt is highly recommended to read the original papers [Girshick14]_, [Girshick15]_, [Ren15]_, [He17]_\nto learn more about the ideas behind Mask R-CNN.\nAppendix from [He16]_ and experiment detail from [Lin17]_ may also be useful reference.\n\n.. hint::\n\n    Please first go through this `sphx_glr_build_examples_datasets_mscoco.py` tutorial to\n    setup MSCOCO dataset on your disk.\n\n.. hint::\n\n    You can skip the rest of this tutorial and start training your Mask RCNN model\n    right away by downloading this script:\n\n    :download:`Download train_mask_rcnn.py<../../../scripts/instance/mask_rcnn/train_mask_rcnn.py>`\n\n    Example usage:\n\n    Train a default resnet50_v1b model with COCO dataset on GPU 0:\n\n    .. code-block:: bash\n\n        python train_mask_rcnn.py --gpus 0\n\n    Train on GPU 0,1,2,3:\n\n    .. code-block:: bash\n\n        python train_mask_rcnn.py --gpus 0,1,2,3\n\n    Check the supported arguments:\n\n    .. code-block:: bash\n\n        python train_mask_rcnn.py --help\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset\n-------\n\nMake sure COCO dataset has been set up on your disk.\nThen, we are ready to load training and validation images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.data import COCOInstance\n# typically we use train2017 (i.e. train2014 + minival35k) split as training data\n# COCO dataset actually has images without any objects annotated,\n# which must be skipped during training to prevent empty labels\ntrain_dataset = COCOInstance(splits='instances_train2017', skip_empty=True)\n# and val2014 (i.e. minival5k) test as validation data\nval_dataset = COCOInstance(splits='instances_val2017', skip_empty=False)\n\nprint('Training images:', len(train_dataset))\nprint('Validation images:', len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data transform\n--------------\nWe can read an (image, label, segm) tuple from the training dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_image, train_label, train_segm = train_dataset[6]\nbboxes = train_label[:, :4]\ncids = train_label[:, 4:5]\nprint('image:', train_image.shape)\nprint('bboxes:', bboxes.shape, 'class ids:', cids.shape)\n# segm is a list of polygons which are arrays of points on the object boundary\nprint('masks', [[poly.shape for poly in polys] for polys in train_segm])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the image with boxes and labels:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nfrom gluoncv.utils import viz\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1)\nax = viz.plot_bbox(train_image, bboxes, labels=cids, class_names=train_dataset.classes, ax=ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To actually see the object segmentation, we need to convert polygons to masks\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom gluoncv.data.transforms import mask as tmask\nwidth, height = train_image.shape[1], train_image.shape[0]\ntrain_masks = np.stack([tmask.to_mask(polys, (width, height)) for polys in train_segm])\nplt_image = viz.plot_mask(train_image, train_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now plot the image with boxes, labels and masks\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1)\nax = viz.plot_bbox(plt_image, bboxes, labels=cids, class_names=train_dataset.classes, ax=ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data transforms, i.e. decoding and transformation, are identical to Faster R-CNN\nwith the exception of segmentation polygons as an additional input.\n:py:class:`gluoncv.data.transforms.presets.rcnn.MaskRCNNDefaultTrainTransform`\nconverts the segmentation polygons to binary segmentation mask.\n:py:class:`gluoncv.data.transforms.presets.rcnn.MaskRCNNDefaultValTransform`\nignores the segmentation polygons and returns image tensor and ``[im_height, im_width, im_scale]``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.data.transforms import presets\nfrom gluoncv import utils\nfrom mxnet import nd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "short, max_size = 600, 1000  # resize image to short side 600 px, but keep maximum length within 1000\ntrain_transform = presets.rcnn.MaskRCNNDefaultTrainTransform(short, max_size)\nval_transform = presets.rcnn.MaskRCNNDefaultValTransform(short, max_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "utils.random.seed(233)  # fix seed in this tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "apply transforms to train image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_image2, train_label2, train_masks2 = train_transform(train_image, train_label, train_segm)\nprint('tensor shape:', train_image2.shape)\nprint('box and id shape:', train_label2.shape)\nprint('mask shape', train_masks2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Images in tensor are distorted because they no longer sit in (0, 255) range.\nLet's convert them back so we can see them clearly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt_image2 = train_image2.transpose((1, 2, 0)) * nd.array((0.229, 0.224, 0.225)) + nd.array((0.485, 0.456, 0.406))\nplt_image2 = (plt_image2 * 255).asnumpy().astype('uint8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transform already converted polygons to masks and we plot them directly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "width, height = plt_image2.shape[1], plt_image2.shape[0]\nplt_image2 = viz.plot_mask(plt_image2, train_masks2)\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1)\nax = viz.plot_bbox(plt_image2, train_label2[:, :4],\n                   labels=train_label2[:, 4:5],\n                   class_names=train_dataset.classes,\n                   ax=ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Loader\n-----------\nData loader is identical to Faster R-CNN with the difference of mask input and output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.data.batchify import Tuple, Append\nfrom mxnet.gluon.data import DataLoader\n\nbatch_size = 2  # for tutorial, we use smaller batch-size\nnum_workers = 0  # you can make it larger(if your CPU has more cores) to accelerate data loading\n\ntrain_bfn = Tuple(*[Append() for _ in range(3)])\ntrain_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n                          batchify_fn=train_bfn, last_batch='rollover', num_workers=num_workers)\nval_bfn = Tuple(*[Append() for _ in range(2)])\nval_loader = DataLoader(val_dataset.transform(val_transform), batch_size, shuffle=False,\n                        batchify_fn=val_bfn, last_batch='keep', num_workers=num_workers)\n\nfor ib, batch in enumerate(train_loader):\n    if ib > 3:\n        break\n    print('data 0:', batch[0][0].shape, 'label 0:', batch[1][0].shape, 'mask 0:', batch[2][0].shape)\n    print('data 1:', batch[0][1].shape, 'label 1:', batch[1][1].shape, 'mask 1:', batch[2][1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mask RCNN Network\n-------------------\nIn GluonCV, Mask RCNN network :py:class:`gluoncv.model_zoo.MaskRCNN`\nis inherited from Faster RCNN network :py:class:`gluoncv.model_zoo.FasterRCNN`.\n\n`Gluon Model Zoo <../../model_zoo/index.html>`__ has some Mask RCNN pretrained networks.\nYou can load your favorite one with one simple line of code:\n\n.. hint::\n\n   To avoid downloading models in this tutorial, we set ``pretrained_base=False``,\n   in practice we usually want to load pre-trained imagenet models by setting\n   ``pretrained_base=True``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv import model_zoo\nnet = model_zoo.get_model('mask_rcnn_resnet50_v1b_coco', pretrained_base=False)\nprint(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mask-RCNN has identical inputs but produces an additional output.\n``cids`` are the class labels,\n``scores`` are confidence scores of each prediction,\n``bboxes`` are absolute coordinates of corresponding bounding boxes.\n``masks`` are predicted segmentation masks corresponding to each bounding box\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mxnet as mx\nx = mx.nd.zeros(shape=(1, 3, 600, 800))\nnet.initialize()\ncids, scores, bboxes, masks = net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During training, an additional output is returned:\n``mask_preds`` are per class masks predictions\nin addition to ``cls_preds``, ``box_preds``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mxnet import autograd\nwith autograd.train_mode():\n    # this time we need ground-truth to generate high quality roi proposals during training\n    gt_box = mx.nd.zeros(shape=(1, 1, 4))\n    cls_preds, box_preds, mask_preds, roi, samples, matches, rpn_score, rpn_box, anchors = net(x, gt_box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training losses\n----------------\nThere are one additional losses in Mask-RCNN.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the loss to penalize incorrect foreground/background prediction\nrpn_cls_loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n# the loss to penalize inaccurate anchor boxes\nrpn_box_loss = mx.gluon.loss.HuberLoss(rho=1/9.)  # == smoothl1\n# the loss to penalize incorrect classification prediction.\nrcnn_cls_loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n# and finally the loss to penalize inaccurate proposals\nrcnn_box_loss = mx.gluon.loss.HuberLoss()  # == smoothl1\n# the loss to penalize incorrect segmentation pixel prediction\nrcnn_mask_loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training targets\n----------------\nRPN and RCNN training target are the same as in :doc:`../examples_detection/train_faster_rcnn_voc`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also push RPN targets computation to CPU workers, so network is passed to transforms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_transform = presets.rcnn.MaskRCNNDefaultTrainTransform(short, max_size, net)\n# return images, labels, masks, rpn_cls_targets, rpn_box_targets, rpn_box_masks loosely\nbatchify_fn = Tuple(*[Append() for _ in range(6)])\n# For the next part, we only use batch size 1\nbatch_size = 1\ntrain_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mask targets are generated with the intermediate outputs after rcnn target is generated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for ib, batch in enumerate(train_loader):\n    if ib > 0:\n        break\n    with autograd.train_mode():\n        for data, label, masks, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch):\n            gt_label = label[:, :, 4:5]\n            gt_box = label[:, :, :4]\n            # network forward\n            cls_preds, box_preds, mask_preds, roi, samples, matches, rpn_score, rpn_box, anchors = net(data, gt_box)\n            # generate targets for rcnn\n            cls_targets, box_targets, box_masks = net.target_generator(roi, samples, matches, gt_label, gt_box)\n            # generate targets for mask head\n            mask_targets, mask_masks = net.mask_target(roi, masks, matches, cls_targets)\n            print('data:', data.shape)\n            # box and class labels\n            print('box:', gt_box.shape)\n            print('label:', gt_label.shape)\n            # -1 marks ignored label\n            print('rpn cls label:', rpn_cls_targets.shape)\n            # mask out ignored box label\n            print('rpn box label:', rpn_box_targets.shape)\n            print('rpn box mask:', rpn_box_masks.shape)\n            # rcnn does not have ignored label\n            print('rcnn cls label:', cls_targets.shape)\n            # mask out ignored box label\n            print('rcnn box label:', box_targets.shape)\n            print('rcnn box mask:', box_masks.shape)\n            print('rcnn mask label:', mask_targets.shape)\n            print('rcnn mask mask:', mask_masks.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training loop\n-------------\nAfter we have defined loss function and generated training targets, we can write the training loop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for ib, batch in enumerate(train_loader):\n    if ib > 0:\n        break\n    with autograd.record():\n        for data, label, masks, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch):\n            gt_label = label[:, :, 4:5]\n            gt_box = label[:, :, :4]\n            # network forward\n            cls_preds, box_preds, mask_preds, roi, samples, matches, rpn_score, rpn_box, anchors = net(data, gt_box)\n            # generate targets for rcnn\n            cls_targets, box_targets, box_masks = net.target_generator(roi, samples, matches, gt_label, gt_box)\n            # generate targets for mask head\n            mask_targets, mask_masks = net.mask_target(roi, masks, matches, cls_targets)\n\n            # losses of rpn\n            rpn_score = rpn_score.squeeze(axis=-1)\n            num_rpn_pos = (rpn_cls_targets >= 0).sum()\n            rpn_loss1 = rpn_cls_loss(rpn_score, rpn_cls_targets, rpn_cls_targets >= 0) * rpn_cls_targets.size / num_rpn_pos\n            rpn_loss2 = rpn_box_loss(rpn_box, rpn_box_targets, rpn_box_masks) * rpn_box.size / num_rpn_pos\n\n            # losses of rcnn\n            num_rcnn_pos = (cls_targets >= 0).sum()\n            rcnn_loss1 = rcnn_cls_loss(cls_preds, cls_targets, cls_targets >= 0) * cls_targets.size / cls_targets.shape[0] / num_rcnn_pos\n            rcnn_loss2 = rcnn_box_loss(box_preds, box_targets, box_masks) * box_preds.size / box_preds.shape[0] / num_rcnn_pos\n\n            # loss of mask\n            mask_loss = rcnn_mask_loss(mask_preds, mask_targets, mask_masks) * mask_targets.size / mask_targets.shape[0] / mask_masks.sum()\n\n        # some standard gluon training steps:\n        # autograd.backward([rpn_loss1, rpn_loss2, rcnn_loss1, rcnn_loss2, mask_loss])\n        # trainer.step(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. hint::\n\n  Please checkout the full :download:`training script <../../../scripts/instance/mask_rcnn/train_mask_rcnn.py>` for complete implementation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n.. [Girshick14] Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. CVPR 2014.\n.. [Girshick15] Ross Girshick. Fast {R-CNN}. ICCV 2015.\n.. [Ren15] Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun. Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks. NIPS 2015.\n.. [He16] Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun. Deep Residual Learning for Image Recognition. CVPR 2016.\n.. [Lin17] Tsung-Yi Lin and Piotr Doll\u00e1r and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie. Feature Pyramid Networks for Object Detection. CVPR 2017.\n.. [He17] Kaiming He and Georgia Gkioxari and Piotr Doll\u00e1r and and Ross Girshick. Mask {R-CNN}. ICCV 2017.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}