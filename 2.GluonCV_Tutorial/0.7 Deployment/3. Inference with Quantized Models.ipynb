{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Inference with Quantized Models\n=====================================\n\nThis is a tutorial which illustrates how to use quantized GluonCV\nmodels for inference on Intel Xeon Processors to gain higher performance.\n\nThe following example requires ``GluonCV>=0.4`` and ``MXNet-mkl>=1.5.0b20190314``. Please follow `our installation guide <../../index.html#installation>`__ to install or upgrade GluonCV and nightly build of MXNet if necessary.\n\nIntroduction\n------------\n\nGluonCV delivered some quantized models to improve the performance and reduce the deployment costs for the computer vision inference tasks. In real production, there are two main benefits of lower precision (INT8). First, the computation can be accelerated by the low precision instruction, like Intel Vector Neural Network Instruction (VNNI). Second, lower precision data type would save the memory bandwidth and allow for better cache locality and save the power. The new feature can get up to 2X performance speedup in the current AWS EC2 CPU instances and will reach 4X under the `Intel Deep Learning Boost (VNNI) <https://www.intel.ai/intel-deep-learning-boost/#gs.0ngn54>`_ enabled hardware with less than 0.5% accuracy drop.\n\nPlease checkout `verify_pretrained.py <https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/classification/imagenet/verify_pretrained.py>`_ for imagenet inference\nand `eval_ssd.py <https://raw.githubusercontent.com/dmlc/gluon-cv/master/scripts/detection/ssd/eval_ssd.py>`_ for SSD inference.\n\nPerformance\n-----------\n\nGluonCV supports some quantized classification models and detection models.\nFor the throughput, the target is to achieve the maximum machine efficiency to combine the inference requests together and get the results by one iteration. From the bar-chart, it is clearly that the quantization approach improved the throughput from 1.46X to 2.71X for selected models.\nBelow CPU performance is from AWS EC2 C5.18xlarge with 18 cores.\n\n.. figure:: https://user-images.githubusercontent.com/17897736/54540947-dc08c480-49d3-11e9-9a0d-a97d44f9792c.png\n   :alt: Gluon Quantization Performance\n\n   Gluon Quantization Performance\n\n+-----------------------+----------+------------+------------------+------------------+---------+-----------------+-----------------+\n|  Model                | Dataset  | Batch Size | C5.18xlarge FP32 | C5.18xlarge INT8 | Speedup | FP32 Accuracy   | INT8 Accuracy   |\n+=======================+==========+============+==================+==================+=========+=================+=================+\n| ResNet50 V1           | ImageNet | 128        | 122.02           | 276.72           | 2.27    | 77.21%/93.55%   | 76.86%/93.46%   |\n+-----------------------+----------+------------+------------------+------------------+---------+-----------------+-----------------+\n| MobileNet 1.0         | ImageNet | 128        | 375.33           | 1016.39          | 2.71    | 73.28%/91.22%   | 72.85%/90.99%   |\n+-----------------------+----------+------------+------------------+------------------+---------+-----------------+-----------------+\n| SSD-VGG 300*          | VOC      | 224        | 21.55            | 31.47            | 1.46    | 77.4            | 77.46           |\n+-----------------------+----------+------------+------------------+------------------+---------+-----------------+-----------------+\n| SSD-VGG 512*          | VOC      | 224        | 7.63             | 11.69            | 1.53    | 78.41           | 78.39           |\n+-----------------------+----------+------------+------------------+------------------+---------+-----------------+-----------------+\n| SSD-resnet50_v1 512*  | VOC      | 224        | 17.81            | 34.55            | 1.94    | 80.21           | 80.16           |\n+-----------------------+----------+------------+------------------+------------------+---------+-----------------+-----------------+\n| SSD-mobilenet1.0 512* | VOC      | 224        | 31.13            | 48.72            | 1.57    | 75.42           | 75.04           |\n+-----------------------+----------+------------+------------------+------------------+---------+-----------------+-----------------+\n\nQuantized SSD models are evaluated with ``nms_thresh=0.45``, ``nms_topk=200``.\n\nDemo usage for SSD\n------------------\n\n.. code:: bash\n\n   # with Pascal VOC validation dataset saved on disk\n   python eval_ssd.py --network=vgg16_atrous --quantized --data-shape=300 --batch-size=224 --dataset=voc\n\nUsage:\n\n::\n\n   SYNOPSIS\n            python eval_ssd.py [-h] [--network NETWORK] [--quantized]\n                               [--data-shape DATA_SHAPE] [--batch-size BATCH_SIZE]\n                               [--dataset DATASET] [--num-workers NUM_WORKERS]\n                               [--num-gpus NUM_GPUS] [--pretrained PRETRAINED]\n                               [--save-prefix SAVE_PREFIX]\n\n   OPTIONS\n            -h, --help            show this help message and exit\n            --network NETWORK     Base network name\n            --quantized           use int8 pretrained model\n            --data-shape DATA_SHAPE\n                                    Input data shape\n            --batch-size BATCH_SIZE\n                                    eval mini-batch size\n            --dataset DATASET     eval dataset.\n            --num-workers NUM_WORKERS, -j NUM_WORKERS\n                                    Number of data workers\n            --num-gpus NUM_GPUS   number of gpus to use.\n            --pretrained PRETRAINED\n                                    Load weights from previously saved parameters.\n            --save-prefix SAVE_PREFIX\n                                    Saving parameter prefix\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}